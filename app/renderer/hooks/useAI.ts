/**
 * Knoux Clipboard AI - AI Operations React Hook
 * Custom hook for AI content analysis, enhancement, and processing
 * Generated by Knoux — Abu Retaj
 * Clipboard Intelligence • Desktop Precision • Premium Engineering
 */

import { useState, useCallback, useRef, useEffect } from 'react';
import { llog } from '../../shared/localized-logger';
import { AI, ERROR_CODES } from '../shared/constants';
import { 
  AIAnalysis, 
  AISuggestion, 
  ContentClassification,
  EnhancementOptions as IEnhancementOptions,
  Result,
  AsyncResult 
} from '../shared/types';
import { AIEventType, AIModelType } from '../shared/enums';

/**
 * AI hook configuration
 */
export interface AIHookConfig {
  autoAnalyze: boolean;
  cacheResults: boolean;
  maxConcurrentRequests: number;
  timeoutMs: number;
  retryAttempts: number;
  defaultEnhancementOptions: Partial<IEnhancementOptions>;
}

/**
 * AI analysis state
 */
export interface AIAnalysisState {
  isAnalyzing: boolean;
  isEnhancing: boolean;
  isClassifying: boolean;
  currentOperation?: string;
  progress: number;
  lastAnalysis?: AIAnalysis;
  lastClassification?: ContentClassification;
  error?: string;
  suggestions: AISuggestion[];
  cacheHits: number;
  cacheMisses: number;
}

/**
 * Enhanced content result
 */
export interface EnhancedContent {
  original: string;
  enhanced: string;
  improvements: string[];
  confidence: number;
  processingTimeMs: number;
  suggestions: AISuggestion[];
}

/**
 * Default AI hook configuration
 */
const DEFAULT_CONFIG: AIHookConfig = {
  autoAnalyze: true,
  cacheResults: true,
  maxConcurrentRequests: 3,
  timeoutMs: AI.ANALYSIS_TIMEOUT_MS,
  retryAttempts: 3,
  defaultEnhancementOptions: {
    tone: 'professional',
    complexityLevel: 'intermediate',
    securityCheck: true,
    includeExamples: true,
  },
};

export const useAI = (config: Partial<AIHookConfig> = {}) => {
  const logger = createLogger({ module: 'useAI' });
  
  // Merge config with defaults
  const hookConfig: AIHookConfig = {
    ...DEFAULT_CONFIG,
    ...config,
  };

  // State
  const [state, setState] = useState<AIAnalysisState>({
    isAnalyzing: false,
    isEnhancing: false,
    isClassifying: false,
    progress: 0,
    suggestions: [],
    cacheHits: 0,
    cacheMisses: 0,
  });

  // Refs
  const activeRequests = useRef<Set<string>>(new Set());
  const analysisCache = useRef<Map<string, AIAnalysis>>(new Map());
  const classificationCache = useRef<Map<string, ContentClassification>>(new Map());
  const enhancementCache = useRef<Map<string, EnhancedContent>>(new Map());
  const abortController = useRef<AbortController | null>(null);

  // Cleanup on unmount
  useEffect(() => {
    return () => {
      abortController.current?.abort();
      activeRequests.current.clear();
      llog.debug('AI hook cleanup completed');
    };
  }, []);

  /**
   * Update progress
   */
  const updateProgress = useCallback((progress: number, operation?: string) => {
    setState(prev => ({
      ...prev,
      progress: Math.min(100, Math.max(0, progress)),
      ...(operation && { currentOperation: operation }),
    }));
  }, []);

  /**
   * Start an AI operation
   */
  const startOperation = useCallback((operationType: 'analyze' | 'enhance' | 'classify') => {
    setState(prev => ({
      ...prev,
      isAnalyzing: operationType === 'analyze',
      isEnhancing: operationType === 'enhance',
      isClassifying: operationType === 'classify',
      progress: 0,
      error: undefined,
    }));
    
    // Create new abort controller
    abortController.current?.abort();
    abortController.current = new AbortController();
    
    llog.debug(`AI operation started: ${operationType}`);
  }, []);

  /**
   * End an AI operation
   */
  const endOperation = useCallback((operationType: 'analyze' | 'enhance' | 'classify', error?: string) => {
    setState(prev => ({
      ...prev,
      isAnalyzing: operationType === 'analyze' ? false : prev.isAnalyzing,
      isEnhancing: operationType === 'enhance' ? false : prev.isEnhancing,
      isClassifying: operationType === 'classify' ? false : prev.isClassifying,
      progress: 100,
      ...(error && { error }),
    }));
    
    llog.debug(`AI operation ended: ${operationType}`, { error });
  }, []);

  /**
   * Check if request can be made (concurrency limit)
   */
  const canMakeRequest = useCallback((requestId: string): boolean => {
    if (activeRequests.current.size >= hookConfig.maxConcurrentRequests) {
      llog.warn('Concurrent request limit reached', {
        current: activeRequests.current.size,
        limit: hookConfig.maxConcurrentRequests,
      });
      return false;
    }
    
    if (activeRequests.current.has(requestId)) {
      llog.warn('Duplicate request ID', { requestId });
      return false;
    }
    
    activeRequests.current.add(requestId);
    return true;
  }, [hookConfig.maxConcurrentRequests]);

  /**
   * Complete a request
   */
  const completeRequest = useCallback((requestId: string) => {
    activeRequests.current.delete(requestId);
  }, []);

  /**
   * Generate request ID
   */
  const generateRequestId = useCallback((content: string, operation: string): string => {
    const hash = btoa(content.substring(0, 50)).replace(/[^a-zA-Z0-9]/g, '');
    return `${operation}_${hash}_${Date.now()}`;
  }, []);

  /**
   * Analyze content using AI
   */
  const analyzeContent = useCallback(async (
    content: string,
    context?: any
  ): Promise<Result<AIAnalysis, Error>> => {
    const requestId = generateRequestId(content, 'analyze');
    
    if (!canMakeRequest(requestId)) {
      return {
        success: false,
        error: new Error('Too many concurrent requests'),
      };
    }

    try {
      startOperation('analyze');
      updateProgress(10, 'Preparing analysis...');

      // Check cache
      const cacheKey = `analysis_${btoa(content.substring(0, 100))}`;
      if (hookConfig.cacheResults) {
        const cached = analysisCache.current.get(cacheKey);
        if (cached && isCacheValid(cached.generatedAt)) {
          setState(prev => ({
            ...prev,
            lastAnalysis: cached,
            cacheHits: prev.cacheHits + 1,
          }));
          updateProgress(100, 'Cache hit');
          llog.debug('Analysis cache hit', { cacheKey });
          
          completeRequest(requestId);
          endOperation('analyze');
          
          return { success: true, data: cached };
        }
      }

      setState(prev => ({ ...prev, cacheMisses: prev.cacheMisses + 1 }));

      // Prepare analysis request
      updateProgress(20, 'Sending to AI engine...');
      
      const analysisResult = await window.knoux.analyzeContent(content, {
        context,
        timeout: hookConfig.timeoutMs,
        signal: abortController.current?.signal,
      });

      updateProgress(80, 'Processing results...');

      // Create analysis object
      const analysis: AIAnalysis = {
        id: `analysis_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`,
        itemId: context?.itemId || '',
        summary: analysisResult.summary,
        suggestions: analysisResult.suggestions || [],
        complexity: analysisResult.complexity || 50,
        quality: analysisResult.quality || 70,
        potentialIssues: analysisResult.potentialIssues || [],
        improvements: analysisResult.improvements || [],
        similarPastItems: analysisResult.similarPastItems || [],
        generatedAt: new Date(),
        modelUsed: analysisResult.modelUsed || 'default',
        processingTimeMs: analysisResult.processingTimeMs || 0,
      };

      // Cache the result
      if (hookConfig.cacheResults) {
        analysisCache.current.set(cacheKey, analysis);
        cleanupCache(analysisCache.current);
      }

      // Update state
      setState(prev => ({
        ...prev,
        lastAnalysis: analysis,
        suggestions: analysis.suggestions,
      }));

      updateProgress(100, 'Analysis complete');
      llog.info('Content analysis completed', {
        contentLength: content.length,
        processingTimeMs: analysis.processingTimeMs,
        suggestionCount: analysis.suggestions.length,
      });

      completeRequest(requestId);
      endOperation('analyze');

      return { success: true, data: analysis };

    } catch (error) {
      const errorMessage = error instanceof Error ? error.message : 'Unknown analysis error';
      llog.error('Content analysis failed', error instanceof Error ? error : new Error(errorMessage), {
        contentLength: content.length,
      });

      completeRequest(requestId);
      endOperation('analyze', errorMessage);

      return {
        success: false,
        error: error instanceof Error ? error : new Error(errorMessage),
      };
    }
  }, [
    canMakeRequest,
    completeRequest,
    startOperation,
    endOperation,
    updateProgress,
    generateRequestId,
    hookConfig,
  ]);

  /**
   * Classify content type
   */
  const classifyContent = useCallback(async (
    content: string
  ): Promise<Result<ContentClassification, Error>> => {
    const requestId = generateRequestId(content, 'classify');
    
    if (!canMakeRequest(requestId)) {
      return {
        success: false,
        error: new Error('Too many concurrent requests'),
      };
    }

    try {
      startOperation('classify');
      updateProgress(10, 'Starting classification...');

      // Check cache
      const cacheKey = `classification_${btoa(content.substring(0, 100))}`;
      if (hookConfig.cacheResults) {
        const cached = classificationCache.current.get(cacheKey);
        if (cached) {
          setState(prev => ({
            ...prev,
            lastClassification: cached,
            cacheHits: prev.cacheHits + 1,
          }));
          updateProgress(100, 'Cache hit');
          llog.debug('Classification cache hit', { cacheKey });
          
          completeRequest(requestId);
          endOperation('classify');
          
          return { success: true, data: cached };
        }
      }

      setState(prev => ({ ...prev, cacheMisses: prev.cacheMisses + 1 }));

      // Perform classification
      updateProgress(30, 'Analyzing content type...');
      
      const classificationResult = await window.knoux.classifyContent(content);

      updateProgress(70, 'Processing classification...');

      // Create classification object
      const classification: ContentClassification = {
        primaryType: classificationResult.primaryType,
        secondaryTypes: classificationResult.secondaryTypes || [],
        confidence: classificationResult.confidence || 0.5,
        language: classificationResult.language,
        framework: classificationResult.framework,
        isSensitive: classificationResult.isSensitive || false,
        sensitiveType: classificationResult.sensitiveType,
        patterns: classificationResult.patterns || [],
      };

      // Cache the result
      if (hookConfig.cacheResults) {
        classificationCache.current.set(cacheKey, classification);
        cleanupCache(classificationCache.current);
      }

      // Update state
      setState(prev => ({
        ...prev,
        lastClassification: classification,
      }));

      updateProgress(100, 'Classification complete');
      llog.info('Content classification completed', {
        contentLength: content.length,
        primaryType: classification.primaryType,
        confidence: classification.confidence,
      });

      completeRequest(requestId);
      endOperation('classify');

      return { success: true, data: classification };

    } catch (error) {
      const errorMessage = error instanceof Error ? error.message : 'Unknown classification error';
      llog.error('Content classification failed', error instanceof Error ? error : new Error(errorMessage), {
        contentLength: content.length,
      });

      completeRequest(requestId);
      endOperation('classify', errorMessage);

      return {
        success: false,
        error: error instanceof Error ? error : new Error(errorMessage),
      };
    }
  }, [
    canMakeRequest,
    completeRequest,
    startOperation,
    endOperation,
    updateProgress,
    generateRequestId,
    hookConfig,
  ]);

  /**
   * Enhance content using AI
   */
  const enhanceContent = useCallback(async (
    content: string,
    options: Partial<IEnhancementOptions> = {}
  ): Promise<Result<EnhancedContent, Error>> => {
    const requestId = generateRequestId(content, 'enhance');
    
    if (!canMakeRequest(requestId)) {
      return {
        success: false,
        error: new Error('Too many concurrent requests'),
      };
    }

    try {
      startOperation('enhance');
      updateProgress(5, 'Preparing enhancement...');

      // Merge with default options
      const enhancementOptions: IEnhancementOptions = {
        ...hookConfig.defaultEnhancementOptions,
        ...options,
      };

      // Check cache
      const cacheKey = `enhancement_${btoa(content.substring(0, 100))}_${JSON.stringify(enhancementOptions)}`;
      if (hookConfig.cacheResults) {
        const cached = enhancementCache.current.get(cacheKey);
        if (cached) {
          setState(prev => ({
            ...prev,
            cacheHits: prev.cacheHits + 1,
          }));
          updateProgress(100, 'Cache hit');
          llog.debug('Enhancement cache hit', { cacheKey });
          
          completeRequest(requestId);
          endOperation('enhance');
          
          return { success: true, data: cached };
        }
      }

      setState(prev => ({ ...prev, cacheMisses: prev.cacheMisses + 1 }));

      // Classify content first to determine enhancement strategy
      updateProgress(15, 'Classifying content...');
      const classificationResult = await classifyContent(content);
      
      if (!classificationResult.success) {
        throw classificationResult.error;
      }

      // Perform enhancement
      updateProgress(30, 'Enhancing content...');
      
      const enhancementResult = await window.knoux.enhancePrompt(content, {
        ...enhancementOptions,
        contentType: classificationResult.data.primaryType,
      });

      updateProgress(80, 'Processing enhanced content...');

      if (!enhancementResult.success || !enhancementResult.enhanced) {
        throw new Error(enhancementResult.error || 'Enhancement failed');
      }

      // Generate suggestions
      const suggestions = await generateEnhancementSuggestions(
        content,
        enhancementResult.enhanced,
        classificationResult.data
      );

      // Create enhanced content object
      const enhanced: EnhancedContent = {
        original: content,
        enhanced: enhancementResult.enhanced,
        improvements: enhancementResult.improvements || [],
        confidence: enhancementResult.confidence || 0.7,
        processingTimeMs: enhancementResult.processingTimeMs || 0,
        suggestions,
      };

      // Cache the result
      if (hookConfig.cacheResults) {
        enhancementCache.current.set(cacheKey, enhanced);
        cleanupCache(enhancementCache.current);
      }

      updateProgress(100, 'Enhancement complete');
      llog.info('Content enhancement completed', {
        originalLength: content.length,
        enhancedLength: enhanced.enhanced.length,
        confidence: enhanced.confidence,
        improvements: enhanced.improvements.length,
      });

      completeRequest(requestId);
      endOperation('enhance');

      return { success: true, data: enhanced };

    } catch (error) {
      const errorMessage = error instanceof Error ? error.message : 'Unknown enhancement error';
      llog.error('Content enhancement failed', error instanceof Error ? error : new Error(errorMessage), {
        contentLength: content.length,
        options,
      });

      completeRequest(requestId);
      endOperation('enhance', errorMessage);

      return {
        success: false,
        error: error instanceof Error ? error : new Error(errorMessage),
      };
    }
  }, [
    canMakeRequest,
    completeRequest,
    startOperation,
    endOperation,
    updateProgress,
    generateRequestId,
    classifyContent,
    hookConfig,
  ]);

  /**
   * Generate enhancement suggestions
   */
  const generateEnhancementSuggestions = useCallback(async (
    original: string,
    enhanced: string,
    classification: ContentClassification
  ): Promise<AISuggestion[]> => {
    try {
      const suggestionsResult = await window.knoux.getSuggestions(original, {
        enhancedVersion: enhanced,
        classification,
      });

      return suggestionsResult || [];

    } catch (error) {
      llog.error('Failed to generate enhancement suggestions', error as Error);
      return [];
    }
  }, []);

  /**
   * Summarize text content
   */
  const summarizeText = useCallback(async (
    text: string,
    maxLength?: number
  ): Promise<Result<string, Error>> => {
    const requestId = generateRequestId(text, 'summarize');
    
    if (!canMakeRequest(requestId)) {
      return {
        success: false,
        error: new Error('Too many concurrent requests'),
      };
    }

    try {
      updateProgress(20, 'Starting summarization...');

      const summaryResult = await window.knoux.summarizeText(text, maxLength);

      if (!summaryResult.success || !summaryResult.summary) {
        throw new Error(summaryResult.error || 'Summarization failed');
      }

      updateProgress(100, 'Summarization complete');
      llog.info('Text summarization completed', {
        originalLength: text.length,
        summaryLength: summaryResult.summary.length,
        reduction: ((text.length - summaryResult.summary.length) / text.length * 100).toFixed(1) + '%',
      });

      completeRequest(requestId);

      return { success: true, data: summaryResult.summary };

    } catch (error) {
      const errorMessage = error instanceof Error ? error.message : 'Unknown summarization error';
      llog.error('Text summarization failed', error instanceof Error ? error : new Error(errorMessage), {
        textLength: text.length,
      });

      completeRequest(requestId);

      return {
        success: false,
        error: error instanceof Error ? error : new Error(errorMessage),
      };
    }
  }, [canMakeRequest, completeRequest, updateProgress, generateRequestId]);

  /**
   * Check for sensitive content
   */
  const checkSensitiveContent = useCallback(async (
    content: string
  ): Promise<Result<boolean, Error>> => {
    try {
      const sensitiveResult = await window.knoux.checkSensitive(content);

      return { 
        success: true, 
        data: sensitiveResult.isSensitive || false 
      };

    } catch (error) {
      const errorMessage = error instanceof Error ? error.message : 'Unknown sensitive check error';
      llog.error('Sensitive content check failed', error instanceof Error ? error : new Error(errorMessage));

      return {
        success: false,
        error: error instanceof Error ? error : new Error(errorMessage),
      };
    }
  }, []);

  /**
   * Get AI suggestions for content
   */
  const getSuggestions = useCallback(async (
    content: string,
    context?: any
  ): Promise<Result<AISuggestion[], Error>> => {
    try {
      const suggestions = await window.knoux.getSuggestions(content, context);

      // Update state with new suggestions
      setState(prev => ({
        ...prev,
        suggestions: suggestions || [],
      }));

      return { success: true, data: suggestions || [] };

    } catch (error) {
      const errorMessage = error instanceof Error ? error.message : 'Unknown suggestions error';
      llog.error('Failed to get suggestions', error instanceof Error ? error : new Error(errorMessage));

      return {
        success: false,
        error: error instanceof Error ? error : new Error(errorMessage),
      };
    }
  }, []);

  /**
   * Apply a suggestion
   */
  const applySuggestion = useCallback(async (
    suggestion: AISuggestion,
    currentContent: string
  ): Promise<Result<string, Error>> => {
    try {
      let newContent = currentContent;

      if (suggestion.action) {
        switch (suggestion.action.type) {
          case 'replace':
            if (suggestion.action.content) {
              newContent = suggestion.action.content;
            }
            break;

          case 'append':
            if (suggestion.action.content) {
              newContent = currentContent + '\n' + suggestion.action.content;
            }
            break;

          case 'prepend':
            if (suggestion.action.content) {
              newContent = suggestion.action.content + '\n' + currentContent;
            }
            break;

          case 'execute':
            if (suggestion.action.command) {
              // Execute command through IPC
              llog.debug('Executing suggestion command', { command: suggestion.action.command });
            }
            break;
        }
      }

      // Remove applied suggestion from state
      setState(prev => ({
        ...prev,
        suggestions: prev.suggestions.filter(s => s.id !== suggestion.id),
      }));

      llog.info('Suggestion applied', {
        suggestionId: suggestion.id,
        suggestionType: suggestion.type,
        originalLength: currentContent.length,
        newLength: newContent.length,
      });

      return { success: true, data: newContent };

    } catch (error) {
      const errorMessage = error instanceof Error ? error.message : 'Failed to apply suggestion';
      llog.error('Failed to apply suggestion', error instanceof Error ? error : new Error(errorMessage), {
        suggestionId: suggestion.id,
      });

      return {
        success: false,
        error: error instanceof Error ? error : new Error(errorMessage),
      };
    }
  }, []);

  /**
   * Clear all caches
   */
  const clearCache = useCallback((): void => {
    analysisCache.current.clear();
    classificationCache.current.clear();
    enhancementCache.current.clear();
    
    setState(prev => ({
      ...prev,
      cacheHits: 0,
      cacheMisses: 0,
    }));
    
    llog.debug('AI caches cleared');
  }, []);

  /**
   * Clear specific cache
   */
  const clearCacheByType = useCallback((cacheType: 'analysis' | 'classification' | 'enhancement'): void => {
    switch (cacheType) {
      case 'analysis':
        analysisCache.current.clear();
        break;
      case 'classification':
        classificationCache.current.clear();
        break;
      case 'enhancement':
        enhancementCache.current.clear();
        break;
    }
    
    llog.debug('Cache cleared', { cacheType });
  }, []);

  /**
   * Get cache statistics
   */
  const getCacheStats = useCallback(() => {
    return {
      analysis: analysisCache.current.size,
      classification: classificationCache.current.size,
      enhancement: enhancementCache.current.size,
      hits: state.cacheHits,
      misses: state.cacheMisses,
      hitRate: state.cacheHits + state.cacheMisses > 0 
        ? (state.cacheHits / (state.cacheHits + state.cacheMisses)) * 100 
        : 0,
    };
  }, [state.cacheHits, state.cacheMisses]);

  /**
   * Cancel current AI operations
   */
  const cancelOperations = useCallback((): void => {
    abortController.current?.abort();
    activeRequests.current.clear();
    
    setState(prev => ({
      ...prev,
      isAnalyzing: false,
      isEnhancing: false,
      isClassifying: false,
      progress: 0,
      currentOperation: undefined,
    }));
    
    llog.debug('AI operations cancelled');
  }, []);

  /**
   * Reset AI state
   */
  const reset = useCallback((): void => {
    cancelOperations();
    clearCache();
    
    setState({
      isAnalyzing: false,
      isEnhancing: false,
      isClassifying: false,
      progress: 0,
      suggestions: [],
      cacheHits: 0,
      cacheMisses: 0,
    });
    
    llog.debug('AI hook state reset');
  }, [cancelOperations, clearCache]);

  /**
   * Check if cache entry is valid
   */
  const isCacheValid = (timestamp: Date): boolean => {
    const cacheAge = Date.now() - timestamp.getTime();
    return cacheAge < AI.CACHE_DURATION_MINUTES * 60 * 1000;
  };

  /**
   * Cleanup old cache entries
   */
  const cleanupCache = (cache: Map<string, any>): void => {
    if (cache.size > 100) {
      const keysToDelete: string[] = [];
      
      for (const [key, value] of cache.entries()) {
        if (value.generatedAt && !isCacheValid(value.generatedAt)) {
          keysToDelete.push(key);
        }
      }
      
      keysToDelete.forEach(key => cache.delete(key));
      
      if (keysToDelete.length > 0) {
        llog.debug('Cleaned up old cache entries', { 
          cache: cache.constructor.name,
          count: keysToDelete.length 
        });
      }
    }
  };

  /**
   * Get AI model information
   */
  const getAIModelInfo = useCallback(async (): Promise<{
    modelType: AIModelType;
    isLocal: boolean;
    isReady: boolean;
    capabilities: string[];
  }> => {
    try {
      // This would normally come from IPC
      return {
        modelType: AIModelType.LOCAL_LLAMA,
        isLocal: true,
        isReady: true,
        capabilities: ['analysis', 'enhancement', 'classification', 'summarization'],
      };
    } catch (error) {
      llog.error('Failed to get AI model info', error as Error);
      throw error;
    }
  }, []);

  /**
   * Get current AI status
   */
  const getStatus = useCallback((): {
    isBusy: boolean;
    activeOperations: string[];
    progress: number;
    cacheStats: ReturnType<typeof getCacheStats>;
  } => {
    const activeOperations: string[] = [];
    if (state.isAnalyzing) activeOperations.push('analysis');
    if (state.isEnhancing) activeOperations.push('enhancement');
    if (state.isClassifying) activeOperations.push('classification');
    
    return {
      isBusy: state.isAnalyzing || state.isEnhancing || state.isClassifying,
      activeOperations,
      progress: state.progress,
      cacheStats: getCacheStats(),
    };
  }, [state, getCacheStats]);

  return {
    // State
    state,
    
    // Operations
    analyzeContent,
    classifyContent,
    enhanceContent,
    summarizeText,
    checkSensitiveContent,
    getSuggestions,
    applySuggestion,
    
    // Cache management
    clearCache,
    clearCacheByType,
    getCacheStats,
    
    // Control
    cancelOperations,
    reset,
    
    // Information
    getAIModelInfo,
    getStatus,
    
    // Configuration
    config: hookConfig,
  };
};

export default useAI;

