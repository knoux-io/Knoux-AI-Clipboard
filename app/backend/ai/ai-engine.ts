/**
 * Knoux Clipboard AI - AI Engine Core
 * Manages local and cloud AI models with caching, queuing, and fallback
 * Generated by Knoux - Abu Retaj
 * Clipboard Intelligence • Desktop Precision • Premium Engineering
 */

import { llog } from '../../shared/localized-logger';
import { AI, ERROR_CODES } from '../../shared/constants';
import { AIModelType, AITaskPriority, AIAnalysisStatus } from '../../shared/enums';

/**
 * AI model configuration
 */
export interface AIModelConfig {
  type: AIModelType;
  path: string;
  name: string;
  version: string;
  contextSize: number;
  maxTokens: number;
  temperature: number;
  topP: number;
  frequencyPenalty: number;
  presencePenalty: number;
  isLoaded: boolean;
  isLocal: boolean;
  capabilities: string[];
}

/**
 * AI generation options
 */
export interface AIGenerationOptions {
  maxTokens?: number;
  temperature?: number;
  topP?: number;
  frequencyPenalty?: number;
  presencePenalty?: number;
  stopSequences?: string[];
  context?: string;
  timeout?: number;
  signal?: AbortSignal;
  priority?: AITaskPriority;
}

/**
 * AI completion result
 */
export interface AICompletionResult {
  text: string;
  model: string;
  tokensUsed: number;
  finishReason: string;
  processingTimeMs: number;
  cacheHit: boolean;
}

/**
 * AI analysis request
 */
export interface AIAnalysisRequest {
  id: string;
  content: string;
  task: string;
  options: AIGenerationOptions;
  priority: AITaskPriority;
  timestamp: number;
  resolve: (result: AICompletionResult) => void;
  reject: (error: Error) => void;
  status: AIAnalysisStatus;
}

/**
 * AI engine statistics
 */
export interface AIEngineStats {
  totalRequests: number;
  successfulRequests: number;
  failedRequests: number;
  cacheHits: number;
  cacheMisses: number;
  averageResponseTime: number;
  activeModels: AIModelType[];
  queueSize: number;
  memoryUsage: number;
}

/**
 * Cloud AI provider configuration
 */
export interface CloudAIProvider {
  name: string;
  endpoint: string;
  apiKey: string;
  models: string[];
  rateLimit: number;
  costPerToken: number;
  isAvailable: boolean;
}

export class AIEngine {
  private logger = createLogger({ module: 'ai-engine' });
  private modelConfigs: Map<AIModelType, AIModelConfig> = new Map();
  private activeModel: AIModelType | null = null;
  private requestQueue: AIAnalysisRequest[] = [];
  private processingQueue: Set<string> = new Set();
  private completionCache: Map<string, AICompletionResult> = new Map();
  private cloudProviders: CloudAIProvider[] = [];
  private isInitialized = false;
  private isProcessing = false;
  private stats: AIEngineStats;
  private maxConcurrentRequests = 3;
  private queueProcessorInterval: NodeJS.Timeout | null = null;

  constructor() {
    this.stats = this.createInitialStats();
    this.initializeDefaultModels();
  }

  /**
   * Create initial statistics
   */
  private createInitialStats(): AIEngineStats {
    return {
      totalRequests: 0,
      successfulRequests: 0,
      failedRequests: 0,
      cacheHits: 0,
      cacheMisses: 0,
      averageResponseTime: 0,
      activeModels: [],
      queueSize: 0,
      memoryUsage: 0,
    };
  }

  /**
   * Initialize default AI models
   */
  private initializeDefaultModels(): void {
    this.llog.info('Initializing default AI models');

    // Local Llama model
    this.addModelConfig({
      type: AIModelType.LOCAL_LLAMA,
      path: AI.LOCAL_MODEL_PATH,
      name: 'Llama 2 7B Chat',
      version: '2.0.0',
      contextSize: 4096,
      maxTokens: 2048,
      temperature: 0.7,
      topP: 0.9,
      frequencyPenalty: 0.0,
      presencePenalty: 0.0,
      isLoaded: false,
      isLocal: true,
      capabilities: ['text-completion', 'code-generation', 'summarization', 'classification'],
    });

    // Local GPT-2 model
    this.addModelConfig({
      type: AIModelType.LOCAL_GPT2,
      path: 'models/gpt2',
      name: 'GPT-2 Small',
      version: '1.0.0',
      contextSize: 1024,
      maxTokens: 512,
      temperature: 0.8,
      topP: 0.95,
      frequencyPenalty: 0.1,
      presencePenalty: 0.1,
      isLoaded: false,
      isLocal: true,
      capabilities: ['text-completion', 'summarization'],
    });

    // Cloud OpenAI model
    this.addModelConfig({
      type: AIModelType.CLOUD_OPENAI,
      path: AI.CLOUD_API_ENDPOINT,
      name: 'OpenAI GPT-4',
      version: '4.0.0',
      contextSize: 8192,
      maxTokens: 4096,
      temperature: 0.7,
      topP: 0.9,
      frequencyPenalty: 0.0,
      presencePenalty: 0.0,
      isLoaded: false,
      isLocal: false,
      capabilities: ['text-completion', 'code-generation', 'summarization', 'classification', 'translation'],
    });

    // Cloud Anthropic model
    this.addModelConfig({
      type: AIModelType.CLOUD_ANTHROPIC,
      path: 'https://api.anthropic.com/v1/complete',
      name: 'Claude 2',
      version: '2.0.0',
      contextSize: 100000,
      maxTokens: 4096,
      temperature: 0.7,
      topP: 0.9,
      frequencyPenalty: 0.0,
      presencePenalty: 0.0,
      isLoaded: false,
      isLocal: false,
      capabilities: ['text-completion', 'summarization', 'analysis'],
    });

    this.llog.info('Initialized AI model configurations');
  }

  /**
   * Add model configuration
   */
  private addModelConfig(config: AIModelConfig): void {
    this.modelConfigs.set(config.type, config);
  }

  /**
   * Initialize AI engine
   */
  public async initialize(): Promise<void> {
    if (this.isInitialized) {
      return;
    }

    this.llog.info('Initializing AI engine');

    try {
      // Load configuration
      await this.loadConfiguration();

      // Initialize selected model
      await this.initializeActiveModel();

      // Setup cloud providers
      await this.setupCloudProviders();

      // Start queue processor
      this.startQueueProcessor();

      this.isInitialized = true;
      this.llog.info('AI engine initialized successfully', {
        activeModel: this.activeModel,
        modelCount: this.modelConfigs.size,
      });
    } catch (error) {
      this.llog.error('Failed to initialize AI engine', error as Error);
      throw error;
    }
  }

  /**
   * Load configuration from storage
   */
  private async loadConfiguration(): Promise<void> {
    // In production, this would load from config file
    this.llog.debug('Loading AI engine configuration');
  }

  /**
   * Initialize active model
   */
  private async initializeActiveModel(): Promise<void> {
    // Determine which model to use based on configuration
    // For now, default to local Llama if available
    const localModel = this.modelConfigs.get(AIModelType.LOCAL_LLAMA);

    if (localModel) {
      this.activeModel = AIModelType.LOCAL_LLAMA;
      await this.loadModel(localModel);
    } else {
      // Fallback to cloud model
      this.activeModel = AIModelType.CLOUD_OPENAI;
      this.llog.warn('No local model available, using cloud model');
    }
  }

  /**
   * Load a specific model
   */
  private async loadModel(config: AIModelConfig): Promise<void> {
    if (config.isLoaded) {
      return;
    }

    this.llog.info('Loading AI model', { type: config.type });

    try {
      if (config.isLocal) {
        await this.loadLocalModel(config);
      } else {
        await this.loadCloudModel(config);
      }

      config.isLoaded = true;
      this.stats.activeModels.push(config.type);

      this.llog.info('AI model loaded successfully', { type: config.type });

    } catch (error) {
      this.llog.error('Failed to load AI model', error as Error);
      throw error;
    }
  }

  /**
   * Load local model
   */
  private async loadLocalModel(config: AIModelConfig): Promise<void> {
    this.llog.debug('Loading local model', { path: config.path });

    // Simulate model loading
    await new Promise(resolve => setTimeout(resolve, 1000));

    this.llog.debug('Local model loaded successfully', { type: config.type });
  }

  /**
   * Load cloud model configuration
   */
  private async loadCloudModel(config: AIModelConfig): Promise<void> {
    this.llog.debug('Setting up cloud model', { name: config.name });

    // Simulate cloud connectivity check
    await new Promise(resolve => setTimeout(resolve, 500));

    this.llog.debug('Cloud model setup completed', { type: config.type });
  }

  /**
   * Setup cloud AI providers
   */
  private async setupCloudProviders(): Promise<void> {
    this.cloudProviders = [
      {
        name: 'OpenAI',
        endpoint: AI.CLOUD_API_ENDPOINT,
        apiKey: '', // Would be loaded from configuration
        models: ['gpt-4', 'gpt-3.5-turbo'],
        rateLimit: 60,
        costPerToken: 0.00003,
        isAvailable: false,
      },
      {
        name: 'Anthropic',
        endpoint: 'https://api.anthropic.com/v1/complete',
        apiKey: '',
        models: ['claude-2', 'claude-instant'],
        rateLimit: 30,
        costPerToken: 0.00008,
        isAvailable: false,
      },
      {
        name: 'Google AI',
        endpoint: 'https://generativelanguage.googleapis.com/v1beta/models/gemini-pro:generateContent',
        apiKey: '',
        models: ['gemini-pro'],
        rateLimit: 60,
        costPerToken: 0.000025,
        isAvailable: false,
      },
    ];

    // Check provider availability
    await this.checkProviderAvailability();
  }

  /**
   * Check cloud provider availability
   */
  private async checkProviderAvailability(): Promise<void> {
    for (const provider of this.cloudProviders) {
      try {
        // In production, this would make a test API call
        this.llog.debug('Cloud provider available', { name: provider.name });
        provider.isAvailable = true; // Simulate success
      } catch (error) {
        provider.isAvailable = false;
        this.llog.warn('Cloud provider unavailable', { name: provider.name, error: (error as Error).message });
      }
    }
  }

  /**
   * Start queue processor
   */
  private startQueueProcessor(): void {
    if (this.queueProcessorInterval) {
      clearInterval(this.queueProcessorInterval);
    }

    this.queueProcessorInterval = setInterval(() => {
      this.processQueue();
    }, 100); // Process queue every 100ms

    this.llog.debug('AI request queue processor started');
  }

  /**
   * Process queued requests
   */
  private async processQueue(): Promise<void> {
    if (this.isProcessing || this.processingQueue.size >= this.maxConcurrentRequests) {
      return;
    }

    this.isProcessing = true;

    try {
      // Get next request from queue
      const request = this.getNextRequest();
      if (!request) {
        this.isProcessing = false;
        return;
      }

      // Process the request
      await this.processRequest(request);

    } catch (error) {
      this.llog.error('Queue processing error', error as Error);
    } finally {
      this.isProcessing = false;
    }
  }

  /**
   * Get next request from queue based on priority
   */
  private getNextRequest(): AIAnalysisRequest | null {
    if (this.requestQueue.length === 0) {
      return null;
    }

    // Sort by priority and timestamp
    this.requestQueue.sort((a, b) => {
      if (a.priority !== b.priority) {
        return a.priority - b.priority; // Lower priority number = higher priority
      }
      return a.timestamp - b.timestamp; // Older requests first
    });

    return this.requestQueue.shift() || null;
  }

  /**
   * Process a single request
   */
  private async processRequest(request: AIAnalysisRequest): Promise<void> {
    request.status = AIAnalysisStatus.PROCESSING;
    this.processingQueue.add(request.id);
    this.stats.queueSize = this.requestQueue.length;

    this.llog.debug('Processing AI request', {
      requestId: request.id,
      task: request.task,
      priority: request.priority,
    });

    try {
      // Check cache first
      const cacheKey = this.generateCacheKey(request.content, request.task, request.options);
      const cachedResult = this.completionCache.get(cacheKey);

      if (cachedResult) {
        this.stats.cacheHits++;
        request.status = AIAnalysisStatus.CACHED;

        this.llog.debug('AI request cache hit', { requestId: request.id });
        request.resolve(cachedResult);
        return;
      }

      this.stats.cacheMisses++;

      // Generate completion
      const result = await this.generateCompletionInternal(
        request.content,
        request.task,
        request.options
      );

      // Cache the result
      this.completionCache.set(cacheKey, result);
      this.cleanupCache();

      // Update statistics
      this.stats.successfulRequests++;
      this.stats.totalRequests++;
      this.stats.averageResponseTime = this.updateAverageResponseTime(result.processingTimeMs);

      request.status = AIAnalysisStatus.COMPLETED;
      request.resolve(result);

      this.llog.info('AI request completed successfully', {
        requestId: request.id,
        processingTimeMs: result.processingTimeMs,
        tokensUsed: result.tokensUsed,
      });

    } catch (error) {
      request.status = AIAnalysisStatus.FAILED;
      this.stats.failedRequests++;
      this.stats.totalRequests++;

      const errorMessage = error instanceof Error ? error.message : 'Unknown AI error';
      const aiError = new Error(AI request failed: );

      this.llog.error('AI request failed', aiError, { requestId: request.id });
      request.reject(aiError);

    } finally {
      this.processingQueue.delete(request.id);
      this.stats.queueSize = this.requestQueue.length;
    }
  }

  /**
   * Internal completion generation
   */
  private async generateCompletionInternal(
    content: string,
    task: string,
    options: AIGenerationOptions
  ): Promise<AICompletionResult> {
    const startTime = Date.now();

    if (!this.activeModel) {
      throw new Error('No active AI model');
    }

    const modelConfig = this.modelConfigs.get(this.activeModel);
    if (!modelConfig) {
      throw new Error(Active model not found: );
    }

    try {
      let result: AICompletionResult;

      if (modelConfig.isLocal) {
        result = await this.generateLocalCompletion(content, task, options, modelConfig);
      } else {
        result = await this.generateCloudCompletion(content, task, options, modelConfig);
      }

      result.processingTimeMs = Date.now() - startTime;
      return result;

    } catch (error) {
      // Try fallback model
      return await this.tryFallbackModel(content, task, options, error as Error);
    }
  }

  /**
   * Generate completion using local model
   */
  private async generateLocalCompletion(
    content: string,
    task: string,
    options: AIGenerationOptions,
    modelConfig: AIModelConfig
  ): Promise<AICompletionResult> {
    this.llog.debug('Generating local AI completion', {
      model: modelConfig.name,
      contentLength: content.length,
      task,
    });

    // In production, this would call the actual local model
    // For now, simulate local model processing

    const prompt = this.buildPrompt(content, task, options.context);
    const maxTokens = options.maxTokens || Math.min(modelConfig.maxTokens, prompt.length);

    await new Promise(resolve => setTimeout(resolve, Math.max(100, content.length / 10)));

    // Simulate model response
    const responseText = this.simulateAIResponse(content, task);
    const tokensUsed = Math.ceil(responseText.length / 4); // Approximate token count

    return {
      text: responseText,
      model: modelConfig.name,
      tokensUsed,
      finishReason: 'length',
      processingTimeMs: 0, // Will be set by caller
      cacheHit: false,
    };
  }

  /**
   * Generate completion using cloud model
   */
  private async generateCloudCompletion(
    content: string,
    task: string,
    options: AIGenerationOptions,
    modelConfig: AIModelConfig
  ): Promise<AICompletionResult> {
    this.llog.debug('Generating cloud AI completion', {
      model: modelConfig.name,
      contentLength: content.length,
      task,
    });

    // Find available cloud provider
    const provider = this.cloudProviders.find(p => p.isAvailable);
    if (!provider) {
      throw new Error('No cloud providers available');
    }

    // In production, this would make actual API call
    // For now, simulate API call

    const prompt = this.buildPrompt(content, task, options.context);
    const maxTokens = options.maxTokens || Math.min(modelConfig.maxTokens, prompt.length);

    await new Promise(resolve => setTimeout(resolve, Math.max(200, content.length / 20)));

    // Simulate cloud response
    const responseText = this.simulateAIResponse(content, task);
    const tokensUsed = Math.ceil((prompt.length + responseText.length) / 4);

    return {
      text: responseText,
      model:  - ,
      tokensUsed,
      finishReason: 'stop',
      processingTimeMs: 0, // Will be set by caller
      cacheHit: false,
    };
  }

  /**
   * Try fallback model on failure
   */
  private async tryFallbackModel(
    content: string,
    task: string,
    options: AIGenerationOptions,
    originalError: Error
  ): Promise<AICompletionResult> {
    this.llog.warn('Primary model failed, trying fallback', { error: originalError.message });

    // Try to find a fallback model
    const fallbackModel = this.findFallbackModel();
    if (!fallbackModel) {
      throw new Error(All models failed. Last error: );
    }

    // Switch to fallback model
    const originalModel = this.activeModel;
    this.activeModel = fallbackModel;

    try {
      const modelConfig = this.modelConfigs.get(fallbackModel);
      if (!modelConfig) {
        throw new Error('Fallback model configuration not found');
      }

      if (!modelConfig.isLoaded) {
        await this.loadModel(modelConfig);
      }

      const result = await this.generateCompletionInternal(content, task, options);

      this.llog.info('Fallback model succeeded', {
        originalModel,
        fallbackModel,
        task,
      });

      return result;

    } finally {
      // Restore original model for future requests
      this.activeModel = originalModel;
    }
  }

  /**
   * Find suitable fallback model
   */
  private findFallbackModel(): AIModelType | null {
    const currentModel = this.activeModel;

    // Try cloud models first if current is local
    const currentConfig = this.modelConfigs.get(currentModel!);
    if (currentConfig?.isLocal) {
      const cloudModel = Array.from(this.modelConfigs.values())
        .find(m => !m.isLocal && m.isLoaded);
      if (cloudModel) {
        return cloudModel.type;
      }
    }

    // Try any other loaded model
    for (const [type, config] of this.modelConfigs.entries()) {
      if (type !== currentModel && config.isLoaded) {
        return type;
      }
    }

    return null;
  }

  /**
   * Build prompt for AI model
   */
  private buildPrompt(content: string, task: string, context?: string): string {
    let prompt = '';

    if (context) {
      prompt += Context: \n\n;
    }

    prompt += Task: \n\n;
    prompt += Content:\n\n\n;
    prompt += Please analyze the above content and .;

    return prompt;
  }

  /**
   * Simulate AI response (for development)
   */
  private simulateAIResponse(content: string, task: string): string {
    // Simple simulation based on task type
    switch (task.toLowerCase()) {
      case 'analyze':
        return Analysis: The content appears to be . It contains approximately  words.;

      case 'summarize':
        const sentences = content.split(/[.!?]+/).filter(s => s.trim().length > 0);
        const summary = sentences.slice(0, 2).join('. ');
        return summary || 'Summary not available.';

      case 'enhance':
        return Enhanced version: ... [Enhanced for clarity and conciseness];

      case 'classify':
        if (content.includes('function') || content.includes('const') || content.includes('var')) {
          return 'Classification: JavaScript/TypeScript code';
        } else if (content.includes('def ') || content.includes('import ')) {
          return 'Classification: Python code';
        } else if (content.includes('{') && content.includes('}')) {
          return 'Classification: JSON data';
        } else {
          return 'Classification: Plain text';
        }

      default:
        return Processed content for task "". Original length:  characters.;
    }
  }

  /**
   * Update average response time statistic
   */
  private updateAverageResponseTime(newTime: number): number {
    const currentAverage = this.stats.averageResponseTime;
    const totalRequests = this.stats.successfulRequests + this.stats.failedRequests;

    if (totalRequests <= 1) {
      return newTime;
    }

    return (currentAverage * (totalRequests - 1) + newTime) / totalRequests;
  }

  /**
   * Generate cache key
   */
  private generateCacheKey(content: string, task: string, options: AIGenerationOptions): string {
    const optionsHash = JSON.stringify(options);
    const contentHash = btoa(content.substring(0, 100)).replace(/[^a-zA-Z0-9]/g, '_');
    return ai___;
  }

  /**
   * Cleanup old cache entries
   */
  private cleanupCache(): void {
    if (this.completionCache.size > 1000) {
      const keysToDelete: string[] = [];

      for (const [key, result] of this.completionCache.entries()) {
        // Simple cache cleanup based on size
        if (this.completionCache.size > 1000) {
          keysToDelete.push(key);
        }
      }

      keysToDelete.forEach(key => this.completionCache.delete(key));

      if (keysToDelete.length > 0) {
        this.llog.debug('Cleaned up AI cache', { count: keysToDelete.length });
      }
    }
  }

  // ==================== PUBLIC API ====================

  /**
   * Generate AI completion
   */
  public async generateCompletion(
    prompt: string,
    options: AIGenerationOptions = {}
  ): Promise<AICompletionResult> {
    if (!this.isInitialized) {
      await this.initialize();
    }

    const requestId = req__;
    const priority = options.priority || AITaskPriority.NORMAL;

    this.llog.debug('Queueing AI completion request', {
      requestId,
      promptLength: prompt.length,
      priority,
    });

    return new Promise((resolve, reject) => {
      const request: AIAnalysisRequest = {
        id: requestId,
        content: prompt,
        task: 'generate-completion',
        options,
        priority,
        timestamp: Date.now(),
        resolve,
        reject,
        status: AIAnalysisStatus.PENDING,
      };

      this.requestQueue.push(request);
      this.stats.queueSize = this.requestQueue.length;
    });
  }

  /**
   * Analyze content with AI
   */
  public async analyzeContent(
    content: string,
    options: AIGenerationOptions = {}
  ): Promise<AICompletionResult> {
    return this.generateCompletion(content, {
      ...options,
      context: 'Analyze the following content for quality, issues, and improvements.',
    });
  }

  /**
   * Classify content with AI
   */
  public async classifyContent(
    content: string,
    options: AIGenerationOptions = {}
  ): Promise<AICompletionResult> {
    return this.generateCompletion(content, {
      ...options,
      context: 'Classify the following content type (code, text, data, etc.) and detect any sensitive information.',
    });
  }

  /**
   * Summarize content with AI
   */
  public async summarizeContent(
    content: string,
    options: AIGenerationOptions = {}
  ): Promise<AICompletionResult> {
    return this.generateCompletion(content, {
      ...options,
      maxTokens: options.maxTokens || Math.min(200, Math.ceil(content.length / 10)),
      context: 'Summarize the following content concisely while preserving key information.',
    });
  }

  /**
   * Ensure AI engine is ready
   */
  public async ensureReady(): Promise<void> {
    if (!this.isInitialized) {
      await this.initialize();
    }

    if (!this.activeModel) {
      throw new Error('No active AI model configured');
    }
  }

  /**
   * Get active model information
   */
  public getActiveModel(): string {
    if (!this.activeModel) {
      return 'No active model';
    }

    const config = this.modelConfigs.get(this.activeModel);
    return config ?  () : 'Unknown model';
  }

  /**
   * Switch to different AI model
   */
  public async switchModel(modelType: AIModelType): Promise<void> {
    const modelConfig = this.modelConfigs.get(modelType);
    if (!modelConfig) {
      throw new Error(Model not found: );
    }

    if (!modelConfig.isLoaded) {
      await this.loadModel(modelConfig);
    }

    this.activeModel = modelType;

    this.llog.info('AI model switched', {
      oldModel: this.getActiveModel(),
      newModel: modelConfig.name,
    });
  }

  /**
   * Get available models
   */
  public getAvailableModels(): AIModelConfig[] {
    return Array.from(this.modelConfigs.values());
  }

  /**
   * Get engine statistics
   */
  public getStats(): AIEngineStats {
    return {
      ...this.stats,
      memoryUsage: process.memoryUsage().heapUsed / 1024 / 1024, // MB
    };
  }

  /**
   * Clear completion cache
   */
  public clearCache(): void {
    this.completionCache.clear();
    this.llog.debug('AI completion cache cleared');
  }

  /**
   * Get current queue status
   */
  public getQueueStatus(): {
    pending: number;
    processing: number;
    nextPriority: AITaskPriority | null;
  } {
    const nextRequest = this.requestQueue[0];

    return {
      pending: this.requestQueue.length,
      processing: this.processingQueue.size,
      nextPriority: nextRequest ? nextRequest.priority : null,
    };
  }

  /**
   * Cancel all pending requests
   */
  public cancelAllRequests(): void {
    const cancelledCount = this.requestQueue.length;

    this.requestQueue.forEach(request => {
      request.status = AIAnalysisStatus.SKIPPED;
      request.reject(new Error('Request cancelled'));
    });

    this.requestQueue = [];
    this.stats.queueSize = 0;

    this.llog.info('All AI requests cancelled', { count: cancelledCount });
  }

  /**
   * Check if engine is ready
   */
  public isReady(): boolean {
    return this.isInitialized && this.activeModel !== null;
  }

  /**
   * Shutdown AI engine
   */
  public async shutdown(): Promise<void> {
    this.llog.info('Shutting down AI engine');

    // Cancel queue processor
    if (this.queueProcessorInterval) {
      clearInterval(this.queueProcessorInterval);
      this.queueProcessorInterval = null;
    }

    // Cancel all pending requests
    this.cancelAllRequests();

    // Clear caches
    this.completionCache.clear();

    // Unload models (in production)
    for (const config of this.modelConfigs.values()) {
      if (config.isLoaded && config.isLocal) {
        this.llog.debug(Unloading local model: );
        config.isLoaded = false;
      }
    }

    this.isInitialized = false;
    this.activeModel = null;

    this.llog.info('AI engine shutdown completed');
  }
}




